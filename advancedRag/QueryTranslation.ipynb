{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "49ead1c7-05f6-479a-8494-1bf0c50d8612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
    "os.environ[\"USER_AGENT\"]=\"Tanmay\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "89318e28-7f24-4de7-b6b6-0c466a72bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate,ChatPromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEndpoint,ChatHuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ce5db9c9-d2d4-4312-b5d7-f7a95cb70dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.03,\n",
    ")\n",
    "\n",
    "chat = ChatHuggingFace(llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "83750682-8a9d-4982-b516-9611924cb812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I help you today?', additional_kwargs={}, response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=10, prompt_tokens=4, total_tokens=14), 'model': '', 'finish_reason': 'stop'}, id='run-a046045c-93b2-4fea-99f6-b138b84d7f4f-0')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f5b7737e-38ea-44c0-868e-c314909334f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5da65881-cfe0-4208-86a5-c6b4d30b1634",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(blog_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c65732de-b5a3-479b-84c8-b11f2903ba18",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=HuggingFaceEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "682c779d-70c8-482a-95ff-934f8c9764f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "866ef60b-26f6-4dc8-97f4-33a81a1b0f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "65387c32-1344-4cd6-b172-5e889e545e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries = (\n",
    "    prompt_perspectives\n",
    "    | chat\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4abe948a-f386-47a1-804b-86c8a0a259bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7ca7f88d-818b-47e5-9495-0a3b8889a56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7858e0b4-0c9a-4ccc-8ba1-e25f86234383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnTwo(docs):\n",
    "    return docs[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a410aebd-9abc-4a1c-94e8-578cdded9ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents is a technique called Chain of Thought (CoT) that helps the model solve complex tasks by breaking them down into smaller, simpler steps. This approach involves using prompting techniques that instruct the model \"to think step by step\" during execution. It transforms big tasks into manageable tasks and provides insights into the model\\'s thought process. Using CoT allows LLM agents to better plan and execute complicated tasks more accurately and comprehensively.'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain | returnTwo, \n",
    "     \"question\": RunnablePassthrough()} \n",
    "    | prompt\n",
    "    | chat\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aa2c57-cd99-4e01-ae67-458f2b598d28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
