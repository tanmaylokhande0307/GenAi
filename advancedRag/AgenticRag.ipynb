{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c4ab1b2-3b16-430b-8f80-4572f6e9abb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-dotenv in /home/debian/.local/lib/python3.11/site-packages (1.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af115f39-4776-4cd9-8cfb-344c24519f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tranformers[sentencepiece] (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tranformers[sentencepiece]\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -q langchain langgraph duckduckgo-search torch tranformers[sentencepiece] faiss-cpu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72882ef0-006b-4c8a-8603-dd3896b35722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -q arxiv pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e050d625-d39c-448f-9f31-46e3a1cc632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "HUGGINGFACE_API_TOKEN=os.environ.get(\"HUGGINGFACE_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17ed84a9-a207-4eb9-88c4-635fc9a38b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain_community.llms.huggingface_endpoint import HuggingFaceEndpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "076c7ac6-48e6-4b44-9574-fb2b3559a026",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ArxivLoader(query=\"Retrieval Augmented Generation\",load_max_docs=5).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53437cf6-e3bb-4c6e-9b12-8cdcfe3a94c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/debian/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 5 files:   0%|                                   | 0/5 [00:00<?, ?it/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 695/695 [00:00<00:00, 2.66MB/s]\u001b[A\n",
      "\n",
      "tokenizer_config.json: 100%|███████████████| 1.24k/1.24k [00:00<00:00, 4.96MB/s]\u001b[A\n",
      "\n",
      "config.json: 100%|█████████████████████████████| 706/706 [00:00<00:00, 3.30MB/s]\u001b[A\n",
      "Fetching 5 files:  20%|█████▍                     | 1/5 [00:01<00:06,  1.64s/it]\n",
      "tokenizer.json:   0%|                                | 0.00/711k [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "model_optimized.onnx:   0%|                         | 0.00/66.5M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "tokenizer.json: 100%|█████████████████████████| 711k/711k [00:00<00:00, 763kB/s]\u001b[A\n",
      "\n",
      "\n",
      "model_optimized.onnx:  16%|██▌             | 10.5M/66.5M [00:02<00:10, 5.12MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model_optimized.onnx:  32%|█████           | 21.0M/66.5M [00:04<00:09, 5.05MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model_optimized.onnx:  47%|███████▌        | 31.5M/66.5M [00:06<00:06, 5.03MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model_optimized.onnx:  63%|██████████      | 41.9M/66.5M [00:08<00:04, 5.04MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model_optimized.onnx:  79%|████████████▌   | 52.4M/66.5M [00:10<00:02, 5.04MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model_optimized.onnx:  95%|███████████████▏| 62.9M/66.5M [00:12<00:00, 5.04MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model_optimized.onnx: 100%|████████████████| 66.5M/66.5M [00:13<00:00, 4.95MB/s]\u001b[A\u001b[A\n",
      "Fetching 5 files: 100%|███████████████████████████| 5/5 [00:15<00:00,  3.05s/it]\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=350,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "chunked_documents = text_splitter.split_documents(docs)\n",
    "faiss_vectorstore = FAISS.from_documents(\n",
    "    documents=chunked_documents,\n",
    "    embedding=FastEmbedEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea3fbc12-83e4-43cd-8832-4fccca7432b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = FastEmbedEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2b82dc1-85bf-48e6-8f06-c93ddbdf92c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = faiss_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b385ea64-9821-4b32-adae-8a2a30405afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context. If you cannot answer the question with the context, please respond with 'I don't know':\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c4bc32e-12a6-408b-ad54-f19ba9f3b828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_new_token is not default parameter.\n",
      "                    max_new_token was transferred to model_kwargs.\n",
      "                    Please make sure that max_new_token is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/debian/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    endpoint_url=\"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    task=\"text_generation\",\n",
    "    max_new_token=6096,\n",
    "    huggingfacehub_api_token=HUGGINGFACE_API_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d9d766df-1621-492d-878b-f087ee6f92df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "retrieval_augmented_qa_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}| RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": prompt | llm, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "281bb5ac-72c9-4207-90a8-d12108e504ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: RAG, which stands for Retrieval Augmented Generation, is a method used to complement the capabilities of large language models (LLMs) by retrieving and utilizing relevant external knowledge. It consists of three primary components: Tool Retrieval, Plan Generation, and Execution. The goal is to improve the accuracy and performance of LLMs by providing them with the right tools and context for the task at hand.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is RAG?\"\n",
    "\n",
    "result = retrieval_augmented_qa_chain.invoke({\"question\" : question})\n",
    "\n",
    "print(result[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d90481-57b4-44f3-ae03-ed07a3e4ee18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
